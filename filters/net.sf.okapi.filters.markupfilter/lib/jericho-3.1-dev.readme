- First you create a StreamedSource object using one of the constructors (same options as normal Source constructors).
 
 
- Then you have to choose whether to specify a fixed buffer or to allow it to use an internal buffer that can expand its capacity if required.  To use a fixed buffer, call the StreamedSource.setBuffer(char[] buffer) method.
 
Fixed buffer
pros:
- guaranteed limit on memory use
- can re-use an existing buffer if processing multiple files
cons:
- processing a document containing a tag with a length exceeding the buffer size results in an unrecoverable BufferOverlowException.
 
Default expandable buffer
pros:
- guaranteed success processing any document
cons:
- buffer can grow arbitrarily large depending on the maximum tag size encountered
 
The maximum tag size is a significant issue because both comments and CDATA sections are regarded as tags, and it is very common for them to contain large text blocks.  I had a look at how both sax and stax handle this issue.  Both provide chunking facilities for handling arbitrarily long text between tags, but for text inside comments and CDATA sections, sax doesn't provide handlers for them at all, and stax uses an expanding buffer.  The fact that stax does it this way made me think it wouldn't be so bad if my implementation did it too.  Finding a better solution would involve more time again, so I'll only look into it if it becomes a problem for people.
 
 
- Next, choose whether large non-tag text content should be written to a specified Writer instead of expanding the buffer.  To do this, call the StreamedSource.setNonTagTextWriter(Writer) method.
 
Documents containing very large normal text sections are much more likely than documents that contain very large commented text or text within explicit CDATA sections, so this is a prudent way of minimising the growth of the expandable buffer and hence memory usage.  
 
If a NonTagTextWriter is used and the parser encounters a normal text segment that is larger than the current buffer (either fixed or expandable), the buffer retains its present size and the Segment is returned, providing information about its begin and end position and allowing parsing to continue.  If however the Segment.toString() method is called, an IllegalStateException is thrown.  The only way to process the text contained in the segment is via the NonTagTextWriter, which will have been fed the segment text as it was parsed.
 
If no NonTagTextWriter is specified, the buffer expands to fit both normal non-tag text as well as tags.  If a fixed buffer is being used, any over-sized tag or text segment results in an unrecoverable BufferOverflowException.
 
 
The recommended configuration is to use the default expandable buffer and specify a NonTagTextWriter to handle text segments.
 
See the file test/src/net/htmlparser/jericho/StreamedSourceTest.java for sample code that uses it.
 
Let me know if this satisfies all of your requirements, or if you think the design could be improved in any way.
 
